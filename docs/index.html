<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SQ Protocol â€” Two Creative Flows</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 820px;
      margin: 2rem auto;
      padding: 0 1rem;
      line-height: 1.6;
      background-color: #fdfdfd;
    }
    h1, h2, h3 {
      color: #111;
    }
    img {
      max-width: 100%;
      border: 1px solid #ddd;
      border-radius: 6px;
      margin: 1rem 0;
    }
    code {
      background: #eee;
      padding: 2px 5px;
      border-radius: 4px;
    }
    form {
      margin-top: 2rem;
      padding: 1rem;
      background: #fafafa;
      border: 1px solid #ccc;
      border-radius: 6px;
    }
    textarea {
      width: 100%;
      padding: 0.5rem;
      font-size: 1rem;
    }
    button {
      margin-top: 0.5rem;
      padding: 0.4rem 1rem;
      font-size: 1rem;
      background-color: #0070f3;
      color: white;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <h1>ğŸ¬ SQ Protocol â€” The Creative Interop Standard</h1>
  <p><strong>SQ (Synthesis Quotient)</strong> enables creative AI tools to operate in sync â€” via shared scene intent and permission schema.</p>

  <h2>ğŸ§  Two Creative Workflow Models</h2>

  <h3>1ï¸âƒ£ Automated Chain (Integrated)</h3>
  <p>Fully integrated toolchain where agents read <code>scene.json</code> and <code>permission.json</code> to act in sequence without human handoff:</p>
  <ul>
    <li>GPT generates the scene + script</li>
    <li>Suno reads and produces audio</li>
    <li>MidJourney renders visual layers</li>
    <li>Veo assembles final video, respecting timing + permissions</li>
  </ul>
  <p>This requires direct compatibility between tools and permission-aware agents.</p>

  <h3>2ï¸âƒ£ Manual Handoff (Hybrid)</h3>
  <p>Each step still reads from the same SQ intent schema, but users manually pass the output between tools:</p>
  <ul>
    <li>Download <code>scene.json</code> and <code>permission.json</code> from GPT</li>
    <li>Upload to next tool manually (e.g., Suno, MJ, Veo)</li>
    <li>Ideal for modular experimentation or app-based integration</li>
  </ul>

  <h2>ğŸ“Š Visual Model: Two Options</h2>
  <p><em>Automated (top) and Manual (bottom) models of the SQ creative pipeline:</em></p>
  <img src="ChatGPT%20Image%20Jul%2013,%202025%20at%2011_42_41%20AM.png" alt="Creative Flow Models Diagram">

  <h2>ğŸ“‚ Core Files</h2>
  <ul>
    <li><code>scene.json</code> â€” mood, timing, media, characters</li>
    <li><code>permission.json</code> â€” who can act, when, and with what limits</li>
    <li><code>cue_sheets/</code> â€” optional time-based syncing between tools</li>
  </ul>

  <h2>ğŸ“Œ Version</h2>
  <p><strong>Current Release:</strong> v0.1 â€” July 2025</p>
  <p>GitHub: <a href="https://github.com/STOIQA/sq-protocol" target="_blank">STOIQA/sq-protocol</a></p>

  <h2>ğŸ“š SQIP Proposals</h2>
  <ul>
    <li><strong>SQIP-0002</strong> â€” Permission Protocol Spec</li>
    <li><strong>SQIP-0003</strong> â€” Cue Sheet Synchronization</li>
    <li><strong>SQIP-0004</strong> â€” Dual Flow Architecture (WIP)</li>
  </ul>

  <h2>âœ¨ How It Works</h2>
  <p>Agents or tools check <code>scene.json</code> + <code>permission.json</code>, act based on shared timeline, then hand off output â€” either directly or manually.</p>

  <h2>ğŸ™Œ Get Involved</h2>
  <ul>
    <li>Fork the repo and open PRs</li>
    <li>Propose improvements via SQIPs</li>
    <li>Test interoperability across AI tools</li>
  </ul>

  <form name="feedback" netlify>
    <h2>ğŸ’¬ Feedback</h2>
    <label for="feedback">What do you think? Ideas, suggestions, or bugs?</label><br />
    <textarea name="feedback" rows="4" placeholder="Type your message here..."></textarea><br />
    <button type="submit">Submit</button>
  </form>

  <footer style="margin-top: 2rem; font-size: 0.85rem; color: #888;">
    &copy; 2025 STOIQA â€” Creative infrastructure is open.
  </footer>
</body>
</html>
